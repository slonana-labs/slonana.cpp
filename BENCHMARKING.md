# Slonana.cpp Real Benchmark Comparison System

## Overview

Slonana.cpp uses an **automated real benchmark comparison system** that runs actual performance tests against both Slonana and Anza/Agave validators. All performance metrics shown in documentation are **real, measured results** from automated testing, not estimated or mocked values.

## ü§ñ Automated Benchmark System

### Standalone Benchmark Scripts

The benchmark system now uses **standalone, versioned bash scripts** for improved maintainability and local testing:

**Core Scripts:**
- `scripts/benchmark_agave.sh` - Comprehensive Agave validator benchmarking
- `scripts/benchmark_slonana.sh` - Slonana C++ validator benchmarking  
- `scripts/example_usage.sh` - Usage examples and demos

**Key Features:**
- ‚úÖ **Standalone execution** - Scripts work independently of CI/CD
- ‚úÖ **Comprehensive help** - `--help` flag with detailed usage information
- ‚úÖ **Error handling** - `set -euo pipefail` for robust execution
- ‚úÖ **Argument validation** - Required parameter checking with clear error messages
- ‚úÖ **Local testing** - Run benchmarks on development machines
- ‚úÖ **Pre-commit validation** - Automatic script testing before commits

### GitHub Actions Integration

The benchmark system runs automatically:
- **On every push** to main branches
- **Weekly scheduled runs** (Sundays at 6 AM UTC) 
- **Manual triggers** via workflow dispatch
- **Pull request validation** for performance regressions

**Workflow Enhancement:**
- Uses standalone scripts instead of inline workflow logic
- Better error handling and logging
- Improved maintainability and debugging capabilities

### Real Validator Testing

**Agave Validator Setup:**
- Downloads and builds latest stable Anza/Agave validator
- Initializes real ledger with genesis configuration
- Runs actual validator process with standard configuration

**Slonana Validator Setup:**
- Builds slonana.cpp from source with release optimizations
- Configures equivalent validator settings for fair comparison
- Uses real validator binary, not test mocks

## üèóÔ∏è Benchmark Script Architecture

### Standalone Scripts Design

The benchmark system now uses **standalone, versioned bash scripts** designed for:

**Robustness:**
- `set -euo pipefail` - Fail fast on errors
- Comprehensive argument validation
- Graceful error handling with cleanup
- Signal handling for process termination

**Usability:**
- Comprehensive `--help` documentation
- Verbose logging modes
- Clear exit codes for automation
- Both local and CI/CD usage

**Maintainability:**
- Modular design with clear separation of concerns
- Pre-commit hooks for script validation
- Version controlled with the main codebase
- Extensive inline documentation

### Script Structure

Each benchmark script follows a consistent structure:

```bash
#!/usr/bin/env bash
set -euo pipefail

# Configuration and argument parsing
parse_arguments() { ... }

# Dependency checking
check_dependencies() { ... }

# Environment setup
setup_validator() { ... }

# Validator startup
start_validator() { ... }

# Benchmark execution
run_benchmarks() { ... }

# Cleanup handlers
cleanup_validator() { ... }
trap cleanup_validator EXIT

# Main execution
main() { ... }
```

### Error Handling

Scripts implement comprehensive error handling:

**Exit Codes:**
- `0` - Success
- `1` - General error
- `2` - Invalid arguments
- `3` - Missing dependencies
- `4` - Validator startup failure
- `5` - Benchmark execution failure

**Cleanup:**
- Automatic process cleanup on exit
- Temporary file removal
- Graceful validator shutdown

**Test Environment:**
- GitHub Actions ubuntu-latest runners
- Standardized hardware (CPU cores, memory reported)
- Isolated test runs to minimize interference

**Measured Metrics:**
1. **Transaction Throughput (TPS)** - Real RPC request processing
2. **RPC Response Latency** - Actual API call timing 
3. **Memory Usage** - Live process memory consumption
4. **CPU Utilization** - Real validator process CPU usage

**Test Procedure:**
1. Start validator with identical configurations
2. Wait for full validator initialization 
3. Execute standardized benchmark workload
4. Measure system metrics during load testing
5. Record results in machine-readable format

## üìä Current Benchmark Results

> **Live Results:** Performance tables in README.md and docs/index.html are automatically updated with real benchmark data.

**View Latest Results:**
```bash
# Show current benchmark comparison
./scripts/show_benchmark_results.sh

# Raw results data
cat benchmark_comparison.json
```

## Benchmark Categories

### üè¶ Account Operations
- **Account Creation**: Measures speed of creating new program accounts
- **Account Lookup**: Tests account retrieval performance from storage
- **Account Update**: Benchmarks account state modification speed
- **Program Account Query**: Measures filtering and searching program-owned accounts

**Key Metrics**: Operations per second, memory usage, cache hit rates

### üí∏ Transaction Processing
- **Single Instruction**: Basic instruction execution performance
- **Multi-Instruction Transactions**: Complex transaction processing with multiple instructions
- **Compute Intensive**: Heavy computational workloads within compute budget limits
- **Signature Verification**: Cryptographic signature validation speed

**Key Metrics**: Transactions per second (TPS), compute units consumed, latency distribution

### üß± Block Processing
- **Block Creation**: Time to assemble transactions into blocks
- **Block Validation**: Signature verification and consensus rule checking
- **Slot Processing**: Complete slot processing including multiple blocks
- **Merkle Tree Operations**: Block data integrity verification

**Key Metrics**: Blocks per second, validation latency, resource utilization

### üåê RPC Performance
- **getAccountInfo**: Account data retrieval via JSON-RPC
- **getBalance**: Simple balance queries
- **getBlock**: Full block data retrieval
- **getProgramAccounts**: Program-specific account queries

**Key Metrics**: Request latency, throughput, response size, concurrent request handling

### üåç Network Operations
- **Gossip Message Processing**: P2P message validation and propagation
- **Peer Discovery**: Network topology management
- **Transaction Broadcast**: Network-wide transaction dissemination

**Key Metrics**: Message throughput, network latency, peer connection efficiency

### ü§ù Consensus Operations
- **Vote Processing**: Validator vote message handling
- **Leader Schedule Calculation**: Stake-weighted leader selection
- **Proof-of-History Verification**: PoH sequence validation

**Key Metrics**: Consensus latency, vote throughput, PoH verification speed

### üß† Memory Efficiency
- **Memory Allocation**: Dynamic memory management performance
- **Account Cache**: LRU cache efficiency for hot accounts
- **Garbage Collection**: Memory cleanup and optimization

**Key Metrics**: Memory usage, allocation speed, cache hit rates

### ‚ö° Concurrency
- **Parallel Transaction Processing**: Multi-threaded transaction execution
- **Concurrent RPC Requests**: Simultaneous client request handling
- **Thread Pool Efficiency**: Worker thread utilization

**Key Metrics**: Scalability factor, thread efficiency, lock contention

## üöÄ Running Benchmarks Locally

### Quick Start

```bash
# Clone and build the project
git clone https://github.com/slonana-labs/slonana.cpp.git
cd slonana.cpp
cmake -B build && cmake --build build

# Run basic Slonana benchmark
./scripts/benchmark_slonana.sh \
  --ledger /tmp/slonana_ledger \
  --results /tmp/slonana_results

# Run Agave comparison (requires Solana CLI)
./scripts/benchmark_agave.sh \
  --ledger /tmp/agave_ledger \
  --results /tmp/agave_results

# View latest CI results
./scripts/show_benchmark_results.sh
```

## üèóÔ∏è Manual Cluster Benchmarking

For comprehensive performance testing and development work, you can set up and benchmark a complete Solana cluster manually. This provides more control and deeper insights than the automated scripts.

### Prerequisites Setup

First, set up the Rust toolchain and Solana source code:

```bash
# Setup Rust, Cargo and system packages as described in the Solana README
curl --proto '=https' --tlsv1.2 -sSf https://sh.rustup.rs | sh
source ~/.cargo/env

# Install required system dependencies
sudo apt-get update
sudo apt-get install -y build-essential libssl-dev libudev-dev pkg-config

# Clone Solana source code
git clone https://github.com/solana-labs/solana.git
cd solana

# For stability, checkout the latest release
TAG=$(git describe --tags $(git rev-list --tags --max-count=1))
git checkout $TAG
echo "Using Solana version: $TAG"
```

### Configuration Setup

Ensure important programs such as the vote program are built before any nodes are started:

```bash
# Use release build for good performance
cargo build --release

# Set environment variable to enforce release builds
export CARGO_BUILD_PROFILE=release

# For profiling, use release-with-debug profile
# cargo build --profile release-with-debug
# export CARGO_BUILD_PROFILE=release-with-debug

# For debug builds without optimizations, just use:
# cargo build
# (and don't set CARGO_BUILD_PROFILE)
```

### Network Initialization

Initialize the network with a genesis ledger:

```bash
# Generate genesis ledger
./multinode-demo/setup.sh
```

### Faucet Setup

Start the faucet to provide test tokens for transactions:

```bash
# Start faucet in a separate terminal
./multinode-demo/faucet.sh
```

The faucet delivers "air drops" (free tokens) to requesting clients for test transactions.

### Singlenode Testnet

For high-level development work like smart contract experimentation:

```bash
# Ensure UDP ports 8000-10000 are open
# Get your machine's IP address
IP_ADDRESS=$(hostname -I | awk '{print $1}')
echo "Bootstrap validator will run on: $IP_ADDRESS"

# Start bootstrap validator in a separate terminal
./multinode-demo/bootstrap-validator.sh

# Wait for "leader ready..." message before proceeding
```

### Multinode Testnet

For consensus work and full performance testing:

```bash
# After starting a leader node, spin up additional validators in separate terminals
./multinode-demo/validator.sh

# For performance-enhanced validator on Linux with CUDA 10.0
./fetch-perf-libs.sh
SOLANA_CUDA=1 ./multinode-demo/bootstrap-validator.sh
SOLANA_CUDA=1 ./multinode-demo/validator.sh
```

**Testnet Variations:**
- **Rust-only singlenode**: For smart contract development
- **Enhanced singlenode**: For transaction pipeline optimization  
- **Rust-only multinode**: For consensus work
- **Enhanced multinode**: To reproduce TPS metrics

### Client Demo and Benchmarking

Run actual transaction load testing:

```bash
# Run client demo against localhost
./multinode-demo/bench-tps.sh

# The client will:
# - Spin up several threads
# - Send 500,000 transactions as quickly as possible
# - Ping testnet periodically for processed transaction counts
# - Intentionally flood network with UDP packets
# - Ensure testnet has opportunity to reach 710k TPS
# - Show TPS measurements for each validator node
```

### Performance Measurement

The benchmark client provides real-time metrics:

- **Transaction Throughput**: TPS measurements per validator
- **Network Efficiency**: UDP packet handling under load
- **Processing Latency**: Time from submission to confirmation
- **Resource Usage**: CPU, memory, and network utilization

### Debugging and Monitoring

Enable detailed logging for troubleshooting:

```bash
# Enable info everywhere and debug in specific modules
export RUST_LOG=solana=info,solana::banking_stage=debug

# Enable SBF program logging
export RUST_LOG=solana_bpf_loader=trace

# For performance-related logging
export RUST_LOG=solana=info

# Log levels:
# - debug: Infrequent debug messages
# - trace: Potentially frequent messages  
# - info: Performance-related logging
```

### Process Debugging

Attach to running processes for deeper analysis:

```bash
# The leader process is named agave-validator
sudo gdb
attach $(pgrep agave-validator)
```

### Network Configuration

Before starting validators, ensure proper network setup:

```bash
# Open required UDP ports (8000-10000)
sudo ufw allow 8000:10000/udp

# Check firewall status
sudo ufw status

# Verify network connectivity
netstat -tulpn | grep :8000
```

### Custom Benchmark Configurations

Modify test parameters for specific benchmarking needs:

```bash
# Custom TPS targets
./multinode-demo/bench-tps.sh --tx-count 1000000 --thread-batch-sleep-ms 1

# Specific transaction types
./multinode-demo/bench-tps.sh --account-mode account-creation
./multinode-demo/bench-tps.sh --account-mode account-query

# Network stress testing
./multinode-demo/bench-tps.sh --sustained
```

### Cluster Health Monitoring

Monitor cluster status during benchmarking:

```bash
# Check cluster status
solana cluster-version
solana validators
solana slot

# Monitor gossip network
solana gossip

# Check validator logs
tail -f ~/.local/share/solana/install/active_release/bin/agave-validator.log
```

### Results Collection

Collect comprehensive benchmark data:

```bash
# Validator metrics
./multinode-demo/metrics.sh

# Network analysis
ss -tuln | grep 8000

# Resource usage
htop
iostat -x 1
vmstat 1

# Save results
./multinode-demo/bench-tps.sh > benchmark_results_$(date +%Y%m%d_%H%M%S).log
```

This manual setup provides complete control over the benchmarking environment and enables detailed performance analysis that complements the automated testing system.

### Automated Script Usage Examples

**Basic benchmark with custom duration:**
```bash
./scripts/benchmark_slonana.sh \
  --ledger /tmp/ledger \
  --results /tmp/results \
  --test-duration 120 \
  --verbose
```

**Bootstrap-only mode (setup testing):**
```bash
./scripts/benchmark_slonana.sh \
  --ledger /tmp/ledger \
  --results /tmp/results \
  --bootstrap-only
```

**Placeholder mode (when binary not available):**
```bash
./scripts/benchmark_slonana.sh \
  --ledger /tmp/ledger \
  --results /tmp/results \
  --use-placeholder
```

**Custom ports and binary paths:**
```bash
./scripts/benchmark_agave.sh \
  --ledger /tmp/ledger \
  --results /tmp/results \
  --validator-bin /usr/local/bin/agave-validator \
  --rpc-port 9899 \
  --gossip-port 9001
```

### Pre-commit Testing

The system includes pre-commit hooks for script validation:

```bash
# Manual pre-commit test
.git/hooks/pre-commit

# Install pre-commit framework (optional)
pip install pre-commit
pre-commit install
```

### Script Help

All scripts provide comprehensive help:

```bash
./scripts/benchmark_agave.sh --help
./scripts/benchmark_slonana.sh --help
./scripts/example_usage.sh
```

### Legacy Component Benchmarks

### Manual Execution
```bash
# Build benchmarks
mkdir build && cd build
cmake -DCMAKE_BUILD_TYPE=Release ..
make slonana_benchmarks

# Run benchmarks
./slonana_benchmarks
```

### Configuration Options
```bash
# Run specific benchmark categories
./slonana_benchmarks --category=account
./slonana_benchmarks --category=rpc
./slonana_benchmarks --category=consensus

# Adjust iteration counts
./slonana_benchmarks --iterations=10000

# Memory profiling mode
./slonana_benchmarks --profile-memory
```

## Performance Targets

### Expected Performance Ranges

| Operation Category | Target Throughput | Acceptable Latency |
|-------------------|------------------|-------------------|
| Account Operations | 5,000-15,000 ops/sec | < 100Œºs |
| Transaction Processing | 1,000-5,000 TPS | < 500Œºs |
| RPC Requests | 3,000-10,000 req/sec | < 50ms |
| Block Processing | 50-200 blocks/sec | < 100ms |
| Vote Processing | 2,000-8,000 votes/sec | < 10ms |

### Comparison with Anza/Agave

The benchmark suite provides direct performance comparison with the official Anza/Agave validator implementation:

```
Performance Comparison (slonana.cpp vs anza/agave):
Operation                      | Slonana    | Agave*     | Ratio      | Status
Account Creation               | 12,500op/s | 8,000op/s  | 1.56x      | üöÄ FASTER
Transaction Processing         | 3,200op/s  | 2,500op/s  | 1.28x      | üöÄ FASTER
RPC getAccountInfo             | 8,900op/s  | 5,000op/s  | 1.78x      | üöÄ FASTER
Block Validation               | 145op/s    | 100op/s    | 1.45x      | üöÄ FASTER
Vote Processing                | 6,100op/s  | 3,000op/s  | 2.03x      | üöÄ FASTER
```

## Optimization Insights

### Performance Bottlenecks
1. **Signature Verification**: CPU-intensive cryptographic operations
2. **Account Lookup**: Storage I/O and cache efficiency
3. **Network Serialization**: Protocol encoding/decoding overhead
4. **Memory Allocation**: Frequent allocation/deallocation patterns

### Optimization Strategies
1. **Batch Processing**: Group similar operations for efficiency
2. **Cache Optimization**: Implement intelligent account caching
3. **Parallel Execution**: Leverage multi-core processing
4. **Memory Pooling**: Reduce allocation overhead

## Hardware Requirements

### Minimum Specifications
- **CPU**: 4 cores, 2.5GHz
- **Memory**: 8GB RAM
- **Storage**: SSD with 1000 IOPS
- **Network**: 1Gbps connection

### Recommended Specifications
- **CPU**: 16 cores, 3.0GHz+ (Intel Xeon or AMD EPYC)
- **Memory**: 32GB+ RAM
- **Storage**: NVMe SSD with 10,000+ IOPS
- **Network**: 10Gbps connection

### Benchmark Environment
Benchmarks should be run on dedicated hardware to ensure consistent results:
- No other resource-intensive processes
- Stable network conditions
- Consistent CPU frequency (disable turbo boost for repeatability)

## Result Analysis

### Output Files
- `benchmark_output.txt`: Human-readable benchmark results
- `benchmark_results.json`: Machine-readable performance data
- `memory_profile.log`: Memory usage analysis (if enabled)

### Key Performance Indicators (KPIs)
1. **Transaction Throughput**: Total TPS capacity
2. **RPC Latency**: Average response time for client requests
3. **Memory Efficiency**: RAM usage per operation
4. **CPU Utilization**: Processor efficiency
5. **Network Bandwidth**: Data transfer efficiency

### Performance Classification
- **üöÄ EXCELLENT** (>10,000 avg ops/sec): Production ready
- **‚úÖ GOOD** (5,000-10,000 avg ops/sec): Suitable for most use cases
- **‚ö†Ô∏è FAIR** (1,000-5,000 avg ops/sec): Room for optimization
- **‚ùå POOR** (<1,000 avg ops/sec): Significant optimization needed

## Continuous Integration

### Automated Benchmarking
Benchmarks are integrated into the CI/CD pipeline to track performance regressions:

```yaml
- name: Run Performance Benchmarks
  run: |
    ./run_benchmarks.sh
    # Upload results to performance tracking system
```

### Performance Monitoring
- Automatic alerts on performance degradation
- Historical performance trend analysis
- Comparison with previous releases

## Extending Benchmarks

### Adding New Benchmarks
1. Implement benchmark function in `test_benchmarks.cpp`
2. Add to appropriate category in `BenchmarkSuite`
3. Update documentation and expected performance targets

### Custom Benchmark Categories
```cpp
void benchmark_custom_operations() {
    std::cout << "\nüîß CUSTOM OPERATIONS BENCHMARKS" << std::endl;
    
    auto custom_result = measure_performance("Custom Operation", [&]() {
        // Your custom benchmark code here
    }, 1000);
    
    results_.push_back(custom_result);
    custom_result.print();
}
```

## Troubleshooting

### Manual Cluster Issues

**Network Setup Problems:**
```bash
# Verify UDP ports are open
sudo netstat -tulpn | grep :8000
sudo ufw status

# Check if validator can bind to ports
lsof -i :8899  # RPC port
lsof -i :8001  # Gossip port
```

**Validator Startup Issues:**
```bash
# Check validator logs
tail -f ~/.local/share/solana/install/active_release/bin/agave-validator.log

# Verify genesis configuration
solana config get
solana genesis-hash

# Check cluster connectivity
solana cluster-version
solana validators
```

**Performance Issues:**
```bash
# Monitor system resources during benchmarking
htop
iostat -x 1  
iotop
nethogs

# Check for memory pressure
free -h
dmesg | grep -i memory

# Verify CPU frequency scaling
cat /proc/cpuinfo | grep MHz
```

**Transaction Processing Issues:**
```bash
# Check transaction pool status
solana transaction-count

# Monitor slot progression
watch -n 1 'solana slot'

# Verify faucet connectivity
solana airdrop 1 --keypair ~/.config/solana/id.json
```

### Common Issues
1. **Low Performance**: Check system resources and background processes
2. **Inconsistent Results**: Ensure stable test environment
3. **Build Failures**: Verify compiler and dependency versions
4. **Memory Issues**: Monitor available RAM during benchmarks

### Debug Mode
```bash
# Enable debug output
./slonana_benchmarks --debug

# Verbose logging
./slonana_benchmarks --verbose

# Single-threaded mode for debugging
./slonana_benchmarks --single-thread
```

## üêõ Local Debugging Guide

### Debugging Benchmark Script Execution

When running benchmark scripts locally, use this systematic debugging approach:

#### Step 1: Basic Script Validation
```bash
# Check script syntax and help
./scripts/benchmark_agave.sh --help

# Verify script permissions
ls -la ./scripts/benchmark_agave.sh
chmod +x ./scripts/benchmark_agave.sh  # If needed
```

#### Step 2: Dependency Debugging
```bash
# Run dependency check first
./scripts/benchmark_agave.sh --ledger test_ledgers/agave --results benchmark_results/agave --bootstrap-only --verbose

# Check PATH configuration
echo $PATH
which agave-validator
which solana
which solana-keygen

# Common PATH fix for Solana CLI
export PATH="$HOME/.local/share/solana/install/active_release/bin:$PATH"
```

#### Step 3: Verbose Mode Debugging
```bash
# Enable maximum verbosity for troubleshooting
./scripts/benchmark_agave.sh \
    --ledger test_ledgers/agave \
    --results benchmark_results/agave \
    --validator-bin agave-validator \
    --test-duration 60 \
    --verbose \
    --bootstrap-only  # Test setup without full benchmark
```

#### Step 4: Component Testing
```bash
# Test individual components
# 1. Directory creation
mkdir -p test_ledgers/agave benchmark_results/agave

# 2. Keypair generation
solana-keygen new --no-bip39-passphrase --silent --outfile benchmark_results/agave/test-keypair.json

# 3. Validator binary test
agave-validator --help

# 4. Genesis creation test
solana-genesis --bootstrap-validator benchmark_results/agave/test-keypair.json test_ledgers/agave
```

### Common Issues and Solutions

#### "Binary not found in PATH" Errors
```bash
# Problem: agave-validator not found
# Solution 1: Install Agave binaries
cargo install agave-validator agave-ledger-tool --locked

# Solution 2: Use bundled Solana CLI binaries
curl --proto '=https' --tlsv1.2 -sSfL https://solana-install.solana.workers.dev | bash
export PATH="$HOME/.local/share/solana/install/active_release/bin:$PATH"

# Verify installation
which agave-validator || echo "‚ùå agave-validator not found"
which solana-keygen || echo "‚ùå solana-keygen not found"
```

#### Validator Startup Failures
```bash
# Check port availability
netstat -tulpn | grep :8899
netstat -tulpn | grep :8001

# Use different ports if needed
./scripts/benchmark_agave.sh \
    --ledger test_ledgers/agave \
    --results benchmark_results/agave \
    --rpc-port 9899 \
    --gossip-port 9001 \
    --verbose
```

#### Permission and Directory Issues
```bash
# Check directory permissions
ls -la test_ledgers/
ls -la benchmark_results/

# Clean and recreate if needed
rm -rf test_ledgers/agave benchmark_results/agave
mkdir -p test_ledgers/agave benchmark_results/agave

# Verify write access
touch test_ledgers/agave/test.txt
touch benchmark_results/agave/test.txt
rm test_ledgers/agave/test.txt benchmark_results/agave/test.txt
```

### Advanced Debugging Techniques

#### Script Error Tracking
```bash
# Run with bash debug mode
bash -x ./scripts/benchmark_agave.sh \
    --ledger test_ledgers/agave \
    --results benchmark_results/agave \
    --verbose

# Monitor system logs during execution
sudo tail -f /var/log/syslog &
./scripts/benchmark_agave.sh [args...]
```

#### Process Monitoring
```bash
# Monitor validator process
ps aux | grep agave-validator
top -p $(pgrep agave-validator)

# Check resource usage
watch -n 1 'ps -p $(pgrep agave-validator) -o pid,%cpu,%mem,cmd'
```

#### Log Analysis
```bash
# Check validator logs
tail -f benchmark_results/agave/agave_validator.log

# Check for common error patterns
grep -i error benchmark_results/agave/agave_validator.log
grep -i "failed\|timeout\|connection" benchmark_results/agave/agave_validator.log
```

### Script Exit Codes Reference

The benchmark scripts use specific exit codes for debugging:

- **0**: Success
- **1**: General error
- **2**: Invalid arguments
- **3**: Missing dependencies  
- **4**: Validator startup failure
- **5**: Benchmark execution failure

```bash
# Check exit code after script failure
echo "Exit code: $?"

# Example debugging based on exit code
if [ $? -eq 3 ]; then
    echo "Dependency issue - check PATH and installations"
elif [ $? -eq 4 ]; then
    echo "Validator startup failed - check ports and permissions"
fi
```

### Network and Connectivity Issues
```bash
# Test localhost connectivity
curl -s http://localhost:8899/health

# Check RPC endpoint
curl -X POST http://localhost:8899 \
  -H "Content-Type: application/json" \
  -d '{"jsonrpc":"2.0","id":1,"method":"getVersion"}'

# Firewall check (if needed)
sudo ufw status
sudo iptables -L INPUT | grep 8899
```

## Contributing

### Performance Improvements
1. Identify bottlenecks using benchmark results
2. Implement optimizations
3. Validate improvements with before/after benchmarks
4. Submit PR with performance analysis

### Benchmark Contributions
1. Propose new benchmark categories
2. Implement benchmark following existing patterns
3. Add documentation and performance targets
4. Ensure cross-platform compatibility

---

For questions about benchmarking or performance optimization, please open an issue with the `performance` label.